**背景简介**
笔记是对于**博士论文《基于模型泛化的图像处理技术研究 》**（2021年）的学习记录总结与扩展，笔记标题所指的第二章，对应该论文**第二章：基于子批移除算法的图像处理技术**

论文作者：孙若琪 上海交通大学
导师：马利庄（杰青），其团队拥有数字媒体与计算机视觉实验室，与腾讯和商汤科技深度合作。在计算机视觉顶会ICCV 2023公布录用名单中，上海交通大学电子信息与电气工程学院计算机系马利庄教授团队共有10篇论文录用。其团队与多家行业领衔的AI公司合作，研发健康码、远程身份核实系统等系列产品

相关链接：
1.[为腾讯健康码辨人脸，上海交大这个团队获上海科技进步特等奖 (thepaper.cn)](https://m.thepaper.cn/newsDetail_forward_12837596)
2.[马利庄团队简介：][数字媒体和计算机视觉实验室 (sjtu.edu.cn)](https://dmcv.sjtu.edu.cn/)
# 第二章 基于子批移除算法的图像处理技术
## 2.1 dropout 简介
**Dropout⽅法（译为“随机失活”或“丢弃层”方法）**：（我的理解：通过**随机删除权重**模拟来代替实际训练）神经⽹络中的全连接层中随机删除掉⼀些权重来形成新的⽹络结构，数据在每次通过不同的⽹络结构训练过程中得到最终的结果，近似模拟组合不同神经⽹络结构的⽅法

消除方差偏移的一种简单方法是在批标准化层之后放置 Dropout
## 2.2 Dropout系列方法
基于 Dropout 的正则化方法，包括 :DropBlock和随机深度
### 2.2.1 dropblock（**区域失活**/块丢弃方法）

* 采用一次删除掉相邻的区域，而不是随机删除掉一些独立的像素，从而可以感知到图像中的空间相关信息，并因此产生更好的训练效果
* 是一种特殊的 Dropout

### 2.2.2 随机深度
* 主要思想是在训练过程中随机丢弃⽹络中的部分层，以此来优化训练速度和性能
* 其效果类似于传统的Dropout⽅法，但作⽤层次不同

由于"通常 Dropout 和 Batch Normalization（批归一化）是神经网络中一种常用的技术，用于加速神经网络的训练过程并提高模型的性能。关于其对应的中文名，存在） 是不能在同一个网络中相邻出现并使用的"(出自2.1 引言)->“是否存在一种算法，可以**消除方差偏移带来的影响**，并且可以在同一个网络中**同时应用 Dropout 和 Batch Normalization**，并吸取它们的优点，对网络的结果有所提升呢？"->子批移除算法

## 2.3 子批移除算法
### 2.3.1 相关概念
* 由于Dropout的干扰导致:"同一个网络中，批标准化（BN）不能和 Dropout 同时使用，否则在训练和测试的过程会发生方差偏移（variance shift)"--2.3节
* **随机深度**:移除了整个网络块，它没有方差偏移问题,然而存在训练过程不稳定和训练时间过长的问题
* 本文提出的子批移除算法(SBDrop)用于应对上述问题(下图为算法概述)

![[Pasted image 20241010200607.png]]
Conv:即“卷积”（Convolution）操作,CNN的核心组件之一，用于从输入图像中提取特征

filters:卷积核

图中的BNN3表示的是一个四维张量，用于描述卷积神经网络（CNN）中的输入数据或特征图的维度。

- B代表批次大小（Batch Size），即一次处理的数据样本数量。
- N*N表示特征图的空间尺寸，其中N为特征图的高度和宽度，即每个样本的特征图在二维空间上的大小。
- 3代表通道数（Channels），对于彩色图像而言，通常指的是RGB三个颜色通道。在更广泛的CNN应用中，这个数值可能代表不同的特征通道数，取决于网络的具体设计和任务需求。

"Calculate Recalibrated(重新校准) Mean":在**随机丢弃特征图中的部分元素后，重新计算剩余特征图的均值**。这个均值用于后续的重构过程，以确保信息的有效利用和模型的稳定性。

"Refill":使用重新计算得到的均值来填充或重构被丢弃的特征部分，以保持特征图的完整性和准确性。这一步骤是SBDrop算法的关键之一，它有助于减少过拟合，提高模型的泛化能力，并增强模型的鲁棒性

BN:Batch Normalization即批量归一化,解决梯度消失和梯度爆炸的问题，提高训练速度和效果。

## 2.4 实验分析
### 2.4.1 算法实现
![[Pasted image 20241010200026.png]]
### 2.4.2 算法 2-1 子批移除算法解释

**激活值（activation value）**，也被称为激活量或激活输出，是神经网络中一个非常重要的概念。在神经网络中，每个神经元（或称为节点、单元）在接收到来自其他神经元的输入信号后，会通过一个激活函数（activation function）对这些输入进行处理，从而生成一个输出值，这个输出值就是该神经元的激活值。
激活值在神经网络中扮演着至关重要的角色，因为它们决定了网络中信息的流动和传递方式。具体来说，激活值可以作为后续神经元或层的输入，影响整个网络的输出和性能。
在深度学习中，激活函数的选择和使用对于网络的训练效果和泛化能力具有重要影响。**常见的激活函数包括Sigmoid、Tanh、ReLU（Rectified Linear Unit）等**，它们各自具有不同的特性和适用场景。例如，ReLU函数因其简单性和有效性而在现代深度学习中得到广泛应用，它能够有效地缓解梯度消失问题，加速网络的训练过程。

**输入参数**：
- 丢弃比率 \(\gamma\)：控制丢弃激活值的比例。
- 训练或测试的状态 \(mode\)：指示当前是训练阶段还是测试阶段。
- 偏移因子 \(k\)：通常为 \(\sqrt{1 - \gamma}\)，用于测试阶段调整激活值。
- 某一层的输出激活值 \(A\)：算法处理的原始激活值。

**算法流程**：

1. **初始化**：
   - 如果处于训练模式（\(mode = Training\)），则执行后续步骤；若处于测试模式，则直接返回 \(kA\)，即使用偏移因子调整后的激活值。

2. **随机丢弃与均值填充**：
   - **随机选择掩模**：在mini-batch维度上，根据Bernoulli分布（参数为\(\gamma\)）随机生成掩模\(M\)。这意味着每个激活值以\(\gamma\)的概率被保留，以\(1-\gamma\)的概率被丢弃（置为0）。
   - **计算剩余均值**：计算未被丢弃的激活值的均值\(\mu_{\mathcal{B}}\)，即所有非零激活值的平均值。
   - **填充校准均值**：对于被丢弃（值为0）的激活值，使用计算出的均值\(\mu_{\mathcal{B}}\)进行填充。这一步骤在图2-1中通过“Refill with Recalibrated Mean”明确展示。

3. **批量归一化**：
   - 将处理后的激活值（包含原始保留的激活值和用均值填充的激活值）送入批量归一化（BN）层，并可能跟随ReLU激活函数，进行后续处理。这一步在图2-1中由“BN&ReLU”表示。

子批移除算法通过随机丢弃部分激活值，并用剩余激活值的均值来填充被丢弃的部分，从而在训练过程中引入随机性，帮助模型学习到更加鲁棒的特征表示，减少过拟合风险，提升模型在未见数据上的表现。图2-1直观地展示了这一算法的数据处理流程，从输入数据经过卷积和滤波，到随机丢弃、均值填充，最后进入批量归一化层，为神经网络训练提供了一种有效的正则化手段。

## 2.5 dropout与SBDorop对比

* Dropout和子批移除算法（SBDrop）的异同：

| 比较维度              | Dropout                      | 子批移除算法（SBDrop）                |
| ----------------- | ---------------------------- | ----------------------------- |
| **移除对象**          | 神经元                          | 激活值                           |
| **移除方式**          | 随机移除整个神经元及其连接                | 在mini-batch维度上随机丢弃部分激活值并填充均值  |
| **移除比例**          | 设定一个固定的丢弃比率，如0.5             | 设定一个固定的丢弃比率，但具体值可调整           |
| **训练阶段**          | 每次迭代中随机丢弃部分神经元               | 每次迭代中随机丢弃部分激活值                |
| **测试阶段**          | 不丢弃任何神经元，但对输出进行缩放以保持一致性      | 不丢弃任何激活值，直接使用原始输出             |
| **正则化效果**         | 通过减少神经元之间的共适应现象来防止过拟合        | 通过随机丢弃激活值并填充均值来引入正则化效果，防止过拟合  |
| **计算成本**          | 可能增加反向传播的计算成本，因为需要跟踪哪些神经元被丢弃 | 在前向传播时增加计算成本，因为需要计算剩余激活值的均值   |
| **适用场景**          | 广泛应用于全连接层、卷积层等，适用于多种神经网络架构   | 主要用于卷积神经网络（CNN）的卷积层，以减少特征图的冗余 |
| **与批量归一化（BN）的关系** | 通常与批量归一化结合使用，以进一步提高模型性能      | 可以在批量归一化之后应用，以进一步减少过拟合风险      |
| **参数共享**          | 丢弃的神经元参数不共享，每次迭代都重新随机选择      | 被丢弃的激活值通过填充均值来保持参数的一致性        |
| **灵活性**           | 丢弃比率是固定的，但可以根据需要调整           | 除了丢弃比率，还可以通过调整填充均值的方式来影响算法效果  |
* 相关链接：
叮！快来看看我和文心一言的奇妙对话～点击链接 https://yiyan.baidu.com/share/K1kGD027Wg?utm_invite_code=bcBVE1fG0IAj95TmPUPR0Q%3D%3D&utm_name=MTgyKioqKioqNzY%3D&utm_fission_type=common -- 文心一言，既能写文案、读文档，又能绘画聊天、写诗做表，你的全能伙伴！